{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader import PVDataLoader\n",
    "from src import data_loader as dl\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# data parameters\n",
    "DAY_INIT = 10\n",
    "DAY_MIN = 8\n",
    "DAY_MAX = 16\n",
    "N_DAYS = 5\n",
    "MINUTE_INTERVAL = 5\n",
    "DAILY_DATA_POINTS = (DAY_MAX - DAY_MIN) * 60 / MINUTE_INTERVAL\n",
    "N_HOURS_PRED = 2\n",
    "N_SYSTEMS = 15\n",
    "RADIUS = 0.35\n",
    "COORDS = (55, -1.5)\n",
    "IDX = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading data\n",
      "==> Loaded data in: 0 m : 8 sec\n",
      "\n",
      "==> Loading data\n",
      "==> Loaded data in: 0 m : 0 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loader = PVDataLoader(n_days=N_DAYS,\n",
    "                    day_init=DAY_INIT,\n",
    "                    n_systems=N_SYSTEMS,\n",
    "                    radius=RADIUS,\n",
    "                    coords=COORDS,\n",
    "                    minute_interval=MINUTE_INTERVAL,\n",
    "                    day_min=DAY_MIN,\n",
    "                    day_max=DAY_MAX,\n",
    "                    folder_name='pv_data',\n",
    "                    file_name_pv='pv_data_clean.csv',\n",
    "                    file_name_location='location_data_clean.csv')\n",
    "\n",
    "time, y = loader.get_time_series()\n",
    "# scale time to be between 0 and 1\n",
    "\n",
    "t_train, y_train, t_test, y_test = dl.train_test_split(time, y, n_hours=N_HOURS_PRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "periodic_time = dl.periodic_mapping(time, DAY_MIN, DAY_MAX, minute_interval=MINUTE_INTERVAL)\n",
    "periodic_train, _, periodic_test, _ = dl.train_test_split(periodic_time, y, n_hours=N_HOURS_PRED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Latent Force Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from gpytorch.mlls.variational_elbo import VariationalELBO\n",
    "from matplotlib import pyplot as plt\n",
    "from os import path\n",
    "from alfi.plot import Plotter1d\n",
    "from lfm.dataset import PV_LFM_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outputs = 1\n",
    "num_latents = 1\n",
    "num_inducing = 100\n",
    "dataset = PV_LFM_Dataset(num_outputs=num_outputs, \n",
    "                         m_observed=y_train[:, :num_outputs],\n",
    "                         f_observed=periodic_train, \n",
    "                         train_t=t_train,\n",
    "                         variance= 0.1 * torch.ones(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alfi.models.variational_lfm import VariationalLFM, TrainMode\n",
    "from alfi.models.ordinary_lfm import OrdinaryLFM\n",
    "from alfi.configuration import VariationalConfiguration\n",
    "from alfi.utilities.torch import is_cuda\n",
    "import lfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "from torch.distributions import Distribution\n",
    "from torchdiffeq import odeint\n",
    "from gpytorch.lazy import DiagLazyTensor\n",
    "from alfi.utilities.torch import is_cuda\n",
    "\n",
    "class OrdinaryLFM(VariationalLFM):\n",
    "    \"\"\"\n",
    "    Variational approximation for an LFM based on an ordinary differential equation (ODE).\n",
    "    Inheriting classes must override the `odefunc` function which encodes the ODE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_outputs,\n",
    "                 gp_model,\n",
    "                 config: VariationalConfiguration,\n",
    "                 initial_state=None,\n",
    "                 **kwargs):\n",
    "        super().__init__(num_outputs, gp_model, config, **kwargs)\n",
    "        self.nfe = 0\n",
    "        self.latent_gp = None\n",
    "        \n",
    "        if initial_state is None:\n",
    "            self.initial_state = torch.zeros(torch.Size([self.num_outputs, 1]), dtype=self.dtype)\n",
    "        else:\n",
    "            self.initial_state = initial_state\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @initial_state.setter\n",
    "    def initial_state(self, value):\n",
    "        value = value.cuda() if is_cuda() else value\n",
    "        self._initial_state = value\n",
    "\n",
    "    def forward(self, t, step_size=1e-1, return_samples=False, **kwargs):\n",
    "        \"\"\"\n",
    "        t : torch.Tensor\n",
    "            Shape (num_times)\n",
    "        h : torch.Tensor the initial state of the ODE\n",
    "            Shape (num_genes, 1)\n",
    "        Returns\n",
    "        -------\n",
    "        Returns evolved h across times t.\n",
    "        Shape (num_genes, num_points).\n",
    "        \"\"\"\n",
    "        self.nfe = 0\n",
    "\n",
    "        t_f = torch.arange(t.min(), t.max()+step_size/3, step_size/3)\n",
    "        t_output = t\n",
    "        h0 = self.initial_state\n",
    "        h0 = h0.repeat(self.config.num_samples).reshape(1, self.config.num_samples, self.num_outputs)\n",
    "\n",
    "        q_f = self.gp_model(t_f)\n",
    "        \n",
    "        self.latent_gp = q_f.rsample(torch.Size([self.config.num_samples])) #.permute(0, 2, 1)  # (S, I, T)\n",
    "   \n",
    "        # Integrate forward from the initial positions h0.\n",
    "        self.t_index = 0\n",
    "        self.last_t = t_f.min() - 1\n",
    "        h_samples = odeint(self.odefunc, h0, t, method='rk4', options=dict(step_size=step_size)) # (T, S, num_outputs, 1)\n",
    "\n",
    "        if return_samples:\n",
    "            return h_samples\n",
    "\n",
    "        dist = self.build_output_distribution(t_output, h_samples)\n",
    "        self.latent_gp = None\n",
    "        return dist\n",
    "\n",
    "    def build_output_distribution(self, t, h_samples) -> Distribution:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            h_samples: shape (T, S, D)\n",
    "        \"\"\"\n",
    "        h_mean = h_samples.mean(dim=1).squeeze(-1).transpose(0, 1)  # shape was (#outputs, #T, 1)\n",
    "        h_var = h_samples.var(dim=1).squeeze(-1).transpose(0, 1) + 1e-7\n",
    "\n",
    "        # TODO: make distribution something less constraining\n",
    "        # TODO possibly make a beta distribution by sampling from posterior\n",
    "        if self.config.latent_data_present:\n",
    "            # todo: make this\n",
    "            f = self.gp_model(t).rsample(torch.Size([self.config.num_samples])).permute(0, 2, 1)\n",
    "            f_mean = f.mean(dim=0)\n",
    "            f_var = f.var(dim=0) + 1e-7\n",
    "            h_mean = torch.cat([h_mean, f_mean], dim=0)\n",
    "            h_var = torch.cat([h_var, f_var], dim=0)\n",
    "\n",
    "        h_covar = DiagLazyTensor(h_var)  # (num_tasks, t, t)\n",
    "        batch_mvn = gpytorch.distributions.MultivariateNormal(h_mean, h_covar)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal.from_batch_mvn(batch_mvn, task_dim=0)\n",
    "\n",
    "    @abstractmethod\n",
    "    def odefunc(self, t, h, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            h: shape (num_samples, num_outputs, 1)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def sample_latents(self, t, num_samples=1):\n",
    "        q_f = self.gp_model(t)\n",
    "        return self.nonlinearity(q_f.sample(torch.Size([num_samples])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhotovoltaicLFM(OrdinaryLFM):\n",
    "    def __init__(self, \n",
    "                num_outputs : int, \n",
    "                gp_model : gpytorch.models.ApproximateGP, # multi-task GP \n",
    "                config: gpytorch.variational.VariationalStrategy,\n",
    "                dataset : lfm.dataset.LFMDataset,\n",
    "                nonlinear : bool = False,\n",
    "                **kwargs):\n",
    "        \n",
    "        super().__init__(num_outputs=num_outputs, \n",
    "                         gp_model=gp_model, \n",
    "                         config=config, \n",
    "                         initial_state=dataset.data[0][1][0], \n",
    "                         **kwargs)\n",
    "        \n",
    "        self.raw_initial_pv = dataset.data[0][1][0]\n",
    "        self.nonlinear = nonlinear\n",
    "        self.true_f = dataset.f_observed\n",
    "        self.raw_decay = torch.nn.Parameter(torch.randn(num_outputs, 1))\n",
    "        self.raw_growth = torch.nn.Parameter(torch.randn(num_outputs, 1))\n",
    "        \n",
    "    @property\n",
    "    def initial_pv(self):\n",
    "        return self.raw_initial_pv\n",
    "    \n",
    "    @initial_pv.setter\n",
    "    def initial_pv(self, value):\n",
    "        value = value.cuda() if is_cuda() else value\n",
    "        self.raw_initial_pv = value\n",
    "    \n",
    "    def initial_state(self):\n",
    "        return self.initial_pv\n",
    "    \n",
    "    @property\n",
    "    def decay(self):\n",
    "        return self.raw_decay\n",
    "    \n",
    "    @decay.setter\n",
    "    def decay(self, value):\n",
    "        value = value.cuda() if is_cuda() else value\n",
    "        self.raw_decay = value\n",
    "    \n",
    "    @property\n",
    "    def growth(self):\n",
    "        return self.raw_growth\n",
    "    \n",
    "    @growth.setter\n",
    "    def growth(self, value):\n",
    "        value = value.cuda() if is_cuda() else value\n",
    "        self.raw_growth = value\n",
    "    \n",
    "    def G(self, f):\n",
    "        if self.nonlinear:\n",
    "            return torch.sigmoid(f).repeat(1, self.num_outputs, 1)\n",
    "        else:\n",
    "            return f\n",
    "\n",
    "    def odefunc(self, t, h):\n",
    "        # TODO ensure that the time passed to \n",
    "        f_latents = self.G(self.latent_gp)\n",
    "        print(f'h shape: {h.shape}')\n",
    "        print(f'f_latents shape: {f_latents.shape}')\n",
    "        print(f'growth shape: {self.growth.shape}')\n",
    "        print(f'decay shape: {self.decay.shape}')\n",
    "        print(f't shape: {t.shape}')\n",
    "        dh = self.growth * h * f_latents - self.decay * h\n",
    "        return dh \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import ApproximateGPBaseModel, MultiTaskBetaGP\n",
    "from lfm.trainers import VariationalTrainer\n",
    "from src.beta_likelihood import BetaLikelihood_MeanParametrization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.kernels import (MaternKernel, \n",
    "                              PeriodicKernel,\n",
    "                              RBFKernel,\n",
    "                              ScaleKernel, \n",
    "                              AdditiveKernel, \n",
    "                              ProductKernel)\n",
    "# TODO add a prior to the period in periodic\n",
    "\n",
    "matern_base = MaternKernel(nu=3/2, \n",
    "                      lengthscale_prior=gpytorch.priors.GammaPrior(2, 8),\n",
    "                      lengthscale_constraint=gpytorch.constraints.Positive()\n",
    "                      )\n",
    "periodic = PeriodicKernel(period_length_prior=gpytorch.priors.GammaPrior(3, 2),\n",
    "                            period_length_constraint=gpytorch.constraints.Positive()\n",
    "                        )\n",
    "scaled_periodic = ScaleKernel(periodic,\n",
    "                                outputscale_prior=gpytorch.priors.GammaPrior(5, 1),\n",
    "                                outputscale_constraint=gpytorch.constraints.Positive()\n",
    "                            )\n",
    "scaled_matern = ScaleKernel(matern_base, \n",
    "                            outputscale_prior=gpytorch.priors.GammaPrior(5, 2),\n",
    "                            outputscale_constraint=gpytorch.constraints.Interval(0.1, 1)\n",
    "                            )\n",
    "product_kernel_matern_periodic = ScaleKernel(periodic * matern_base,\n",
    "                             outputscale_prior = gpytorch.priors.GammaPrior(5, 2),\n",
    "                             outputscale_constraint=gpytorch.constraints.Positive()\n",
    "                            )\n",
    "rbf_kernel = ScaleKernel(RBFKernel())\n",
    "product_kernel_rbf_matern = ScaleKernel(rbf_kernel * scaled_matern)\n",
    "\n",
    "\n",
    "quasi_periodic_rbf = AdditiveKernel(product_kernel_rbf_matern, scaled_matern)\n",
    "quasi_periodic_matern = AdditiveKernel(product_kernel_matern_periodic, scaled_matern)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure that the trainer will receive the parameters from both the interaction terms and the latent GP paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter = 1e-4\n",
    "gp_config = {\n",
    "            'type': 'stochastic',\n",
    "            'name': 'mean_field',\n",
    "            'num_inducing_points': t_train.size(0),\n",
    "            'mean_init_std': 1,\n",
    "            }\n",
    "gp_inputs = {\n",
    "            'X': dataset.t_observed, \n",
    "            'y': dataset.data[0][1], \n",
    "            'mean_module': gpytorch.means.ZeroMean(),\n",
    "            'covar_module': quasi_periodic_matern,\n",
    "            'likelihood': BetaLikelihood_MeanParametrization(scale=10,\n",
    "                                                                correcting_scale=1,\n",
    "                                                                lower_bound=0.10,\n",
    "                                                                upper_bound=0.80),\n",
    "            # 'num_latents' : num_latents,\n",
    "            # 'variational_strategy': 'mt_indep',\n",
    "            'config': gp_config,\n",
    "            'jitter': jitter\n",
    "}\n",
    "gp_model = ApproximateGPBaseModel(**gp_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alfi.configuration import VariationalConfiguration\n",
    "config = VariationalConfiguration(num_samples=10)\n",
    "\n",
    "lfm_model = PhotovoltaicLFM(num_outputs=1,\n",
    "                            gp_model=gp_model,\n",
    "                            config=config,\n",
    "                            dataset=dataset,\n",
    "                            nonlinear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h shape: torch.Size([1, 10, 1])\n",
      "f_latents shape: torch.Size([1, 10, 456])\n",
      "growth shape: torch.Size([1, 1])\n",
      "decay shape: torch.Size([1, 1])\n",
      "t shape: torch.Size([])\n",
      "h shape: torch.Size([1, 10, 456])\n",
      "f_latents shape: torch.Size([1, 10, 456])\n",
      "growth shape: torch.Size([1, 1])\n",
      "decay shape: torch.Size([1, 1])\n",
      "t shape: torch.Size([])\n",
      "h shape: torch.Size([1, 10, 456])\n",
      "f_latents shape: torch.Size([1, 10, 456])\n",
      "growth shape: torch.Size([1, 1])\n",
      "decay shape: torch.Size([1, 1])\n",
      "t shape: torch.Size([])\n",
      "h shape: torch.Size([1, 10, 456])\n",
      "f_latents shape: torch.Size([1, 10, 456])\n",
      "growth shape: torch.Size([1, 1])\n",
      "decay shape: torch.Size([1, 1])\n",
      "t shape: torch.Size([])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (456) at non-singleton dimension 2.  Target sizes: [1, 10, 1].  Tensor sizes: [10, 456]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[39m=\u001b[39m lfm_model(t_train, step_size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gp/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[154], line 68\u001b[0m, in \u001b[0;36mOrdinaryLFM.forward\u001b[0;34m(self, t, step_size, return_samples, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_t \u001b[39m=\u001b[39m t_f\u001b[39m.\u001b[39mmin() \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 68\u001b[0m h_samples \u001b[39m=\u001b[39m odeint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49modefunc, h0, t, method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrk4\u001b[39;49m\u001b[39m'\u001b[39;49m, options\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(step_size\u001b[39m=\u001b[39;49mstep_size)) \u001b[39m# (T, S, num_outputs, 1)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m# self.t_index = None\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m# self.last_t = None\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m return_samples:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gp/lib/python3.11/site-packages/torchdiffeq/_impl/odeint.py:75\u001b[0m, in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     72\u001b[0m solver \u001b[39m=\u001b[39m SOLVERS[method](func\u001b[39m=\u001b[39mfunc, y0\u001b[39m=\u001b[39my0, rtol\u001b[39m=\u001b[39mrtol, atol\u001b[39m=\u001b[39matol, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m     74\u001b[0m \u001b[39mif\u001b[39;00m event_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     solution \u001b[39m=\u001b[39m solver\u001b[39m.\u001b[39;49mintegrate(t)\n\u001b[1;32m     76\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     event_t, solution \u001b[39m=\u001b[39m solver\u001b[39m.\u001b[39mintegrate_until_event(t[\u001b[39m0\u001b[39m], event_fn)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gp/lib/python3.11/site-packages/torchdiffeq/_impl/solvers.py:110\u001b[0m, in \u001b[0;36mFixedGridODESolver.integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mwhile\u001b[39;00m j \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(t) \u001b[39mand\u001b[39;00m t1 \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m t[j]:\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterp \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 110\u001b[0m         solution[j] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_linear_interp(t0, t1, y0, y1, t[j])\n\u001b[1;32m    111\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterp \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcubic\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    112\u001b[0m         f1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(t1, y1)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (456) at non-singleton dimension 2.  Target sizes: [1, 10, 1].  Tensor sizes: [10, 456]"
     ]
    }
   ],
   "source": [
    "out = lfm_model(t_train, step_size=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
