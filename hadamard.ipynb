{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from data import PVWeatherLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parameters\n",
    "DAY_INIT = 40\n",
    "DAY_MIN = 8\n",
    "DAY_MAX = 16\n",
    "N_DAYS = 10\n",
    "MINUTE_INTERVAL = 60\n",
    "DAILY_DATA_POINTS = (DAY_MAX - DAY_MIN) * 60 / MINUTE_INTERVAL\n",
    "N_SYSTEMS = 10\n",
    "# create a tuple of 4 coordinates that form a polygon in the uk\n",
    "# and a circle with a radius of r\n",
    "CIRCLE_COORDS = (55, -1.5)\n",
    "RADIUS = 0.3\n",
    "POLY_COORDS = ((50, -6), (50.5, 1.9), (57.6, -5.5), (58, 1.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading data\n",
      "==> Loaded data in: 0 m : 0 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eirikbaekkelund/Desktop/UCL/Code/Dissertation/data/utils.py:547: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(['season'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "loader = PVWeatherLoader(\n",
    "    # number of days to get data for\n",
    "    n_days=N_DAYS,\n",
    "    # initial day of the data for that season\n",
    "    # look at the data frame to see which day it is\n",
    "    day_init=DAY_INIT,\n",
    "    # number of systems to extract\n",
    "    n_systems=N_SYSTEMS,\n",
    "    coords=CIRCLE_COORDS,\n",
    "    radius=RADIUS,\n",
    "    # the minute interval our data is sampled at \n",
    "    # (e.g. 60 for hourly, 30 for half hourly, 15 for 15 minutes, etc.)\n",
    "    minute_interval=MINUTE_INTERVAL,\n",
    "    # the minimum and maximum hour of the day to use\n",
    "    # (e.g. 8 and 15 for 8am to 3pm)\n",
    "    day_min=DAY_MIN,\n",
    "    day_max=DAY_MAX,\n",
    "    folder_name='pv_data',\n",
    "    file_name='pv_and_weather.csv',\n",
    "    distance_method='circle',\n",
    "    # optionally use a season\n",
    "    season='winter',\n",
    "    # optionally drop series with nan values\n",
    "    drop_nan=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 5]) torch.Size([80])\n",
      "torch.Size([80, 5]) torch.Size([80])\n",
      "torch.Size([80, 5]) torch.Size([80])\n",
      "torch.Size([80, 5]) torch.Size([80])\n",
      "torch.Size([80, 5]) torch.Size([80])\n",
      "torch.Size([80, 5]) torch.Size([80])\n",
      "torch.Size([80, 5]) torch.Size([80])\n",
      "torch.Size([80, 5]) torch.Size([80])\n"
     ]
    }
   ],
   "source": [
    "from data.utils import train_test_split\n",
    "\n",
    "for X, y in loader:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, hour=8, minute_interval=60, day_min=8, day_max=15, n_hours=8)\n",
    "    print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([66, 5]), torch.Size([8, 5]), torch.Size([66]), torch.Size([8]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import wandb\n",
    "from gpytorch.variational import (VariationalStrategy, \n",
    "                                  LMCVariationalStrategy,\n",
    "                                  IndependentMultitaskVariationalStrategy,\n",
    "                                  MeanFieldVariationalDistribution)\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from data.utils import store_gp_module_parameters\n",
    "from typing import Optional\n",
    "\n",
    "class MultitaskGPModel(ApproximateGP):\n",
    "    def __init__(self,\n",
    "                 X : torch.Tensor,\n",
    "                 y : torch.Tensor,\n",
    "                 likelihood : gpytorch.likelihoods.Likelihood = None,\n",
    "                 mean_module : gpytorch.means.Mean = None,\n",
    "                 covar_module : gpytorch.kernels.Kernel = None,\n",
    "                 num_latents : int = 1,\n",
    "                 learn_inducing_locations : bool = False,\n",
    "                 jitter : float = 1e-4,\n",
    "                 task_indices : Optional[torch.Tensor] = None):\n",
    "        # check that num_latents is consistent with the batch_shape of the mean and covar modules\n",
    "        assert num_latents == mean_module.batch_shape[0], 'num_latents must be equal to the batch_shape of the mean module'\n",
    "        assert num_latents == covar_module.batch_shape[0], 'num_latents must be equal to the batch_shape of the covar module'\n",
    "        if task_indices is not None:\n",
    "            assert num_latents == task_indices.max() + 1, 'num_latents must be equal to the number of tasks'\n",
    "            self.task_indices = task_indices\n",
    "\n",
    "        num_tasks = y.size(-1)\n",
    "        \n",
    "        # MeanField constructs a variational distribution for each output dimension\n",
    "        variational_distribution = MeanFieldVariationalDistribution(\n",
    "                                    num_inducing_points=X.size(0), \n",
    "                                    batch_shape=torch.Size([num_latents]),\n",
    "                                    jitter=jitter\n",
    "                                )\n",
    "        \n",
    "        # LMC constructs MultitaskMultivariateNormal from the base var dist\n",
    "        variational_strategy = IndependentMultitaskVariationalStrategy(\n",
    "                            VariationalStrategy(\n",
    "                                    model=self, \n",
    "                                    inducing_points=X, \n",
    "                                    variational_distribution=variational_distribution, \n",
    "                                    learn_inducing_locations=learn_inducing_locations,\n",
    "                                ),\n",
    "                            num_tasks=num_tasks,\n",
    "                            task_dim=-1\n",
    "                        )\n",
    "        \n",
    "        super().__init__(variational_strategy)\n",
    "        \n",
    "        self.mean_module = mean_module\n",
    "        self.covar_module =  covar_module\n",
    "        self.likelihood = likelihood\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def forward(self, x,):\n",
    "    \n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "      \n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    def fit(self, \n",
    "            n_iter : int,\n",
    "            lr : float, \n",
    "            verbose : bool = False,\n",
    "            use_wandb : bool = False):\n",
    "            \n",
    "        self.train()\n",
    "        self.likelihood.train()\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.init(\n",
    "                project ='dissertation',\n",
    "                config={'learning_rate': lr, 'epochs': n_iter}\n",
    "            )\n",
    "        \n",
    "        mll = gpytorch.mlls.VariationalELBO(self.likelihood, self, num_data=self.y.size(0))\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "        print_freq = n_iter // 10\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            output = self(self.X, task_indices=self.task_indices)\n",
    "            loss = -mll(output, self.y, task_indices=self.task_indices)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            if verbose and (i+1) % print_freq == 0:\n",
    "                print(f'Iter {i+1}/{n_iter} - Loss: {loss.item()}')\n",
    "            \n",
    "            if use_wandb:\n",
    "                log_dict = store_gp_module_parameters(self)\n",
    "                log_dict['loss'] = loss.item()\n",
    "                wandb.log(log_dict)\n",
    "            \n",
    "            # if loss is not decreasing for 15 iterations, stop training\n",
    "            if i > 0:\n",
    "                if abs(losses[-2] - losses[-1]) < 1e-6:\n",
    "                    j += 1\n",
    "                    if j == 15:\n",
    "                        print(f'Early stopping at iter {i+1}')\n",
    "                        break\n",
    "                else:\n",
    "                    j = 0\n",
    "        \n",
    "        if use_wandb:\n",
    "            wandb.finish()\n",
    "    \n",
    "    def get_inducing_points(self):\n",
    "        return self.variational_strategy.base_variational_strategy.inducing_points\n",
    "    \n",
    "    def predict_mean(self, dist):\n",
    "        return dist.mean.mean(axis=0)\n",
    "    \n",
    "    def predict_mode(self):\n",
    "        return self.likelihood.mode().mean(axis=0)\n",
    "    \n",
    "    def predict_median(self, samples):\n",
    "        return samples.median(axis=0).values.mean(axis=0)\n",
    "    \n",
    "    def confidence_region(self, samples):\n",
    "        # per MC sample\n",
    "        lower, upper = np.percentile(samples, [2.5, 97.5], axis=0)\n",
    "        # across tasks\n",
    "        lower, upper = lower.mean(axis=0), upper.mean(axis=0)\n",
    "        return lower, upper\n",
    "\n",
    "    def predict(self, x, pred_type='dist'):\n",
    "        \"\"\" \n",
    "        Get the predictions for the given x values.\n",
    "        The prediction type can be one of: dist, median, mean, mode, all or\n",
    "        one can get the posterior predictive distribution.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor\n",
    "            pred_type (str, optional): prediction type. Defaults to 'median'.\n",
    "        \n",
    "        Returns:\n",
    "            dist (torch.distributions.Distribution) if pred_type is 'dist': the posterior predictive distribution\n",
    "            (pred, lower, upper) (torch.Tensor, torch.Tensor, torch.Tensor) if pred_type is 'median', 'mean', 'mode'\n",
    "            \n",
    "            where pred is the prediction of the given type and lower, upper \n",
    "            is the 95% confidence interval from MC sampling from the predictive distribution.\n",
    "    \n",
    "        \"\"\"\n",
    "        assert pred_type in ['dist', 'median', 'mean', 'mode', 'all'], 'pred_type must be one of: dist, median, mean, mode, all'\n",
    "    \n",
    "        if isinstance(self.likelihood, gpytorch.likelihoods.MultitaskGaussianLikelihood):\n",
    "            with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "                dist = self.likelihood(self(x))\n",
    "                mean = dist.mean\n",
    "                lower, upper = dist.confidence_region()\n",
    "\n",
    "                return mean, lower, upper\n",
    "        \n",
    "        elif isinstance(self.likelihood, gpytorch.likelihoods.BetaLikelihood):\n",
    "           with torch.no_grad(), gpytorch.settings.fast_pred_var(), gpytorch.settings.num_likelihood_samples(30):\n",
    "                \n",
    "                dist = self.likelihood(self(x))\n",
    "                if pred_type == 'dist':\n",
    "                    return dist\n",
    "\n",
    "                samples = dist.sample(sample_shape=torch.Size([30]))\n",
    "                lower, upper = self.confidence_region(samples)\n",
    "\n",
    "                if pred_type == 'median':\n",
    "                    median = self.predict_median(samples)\n",
    "                    return median, lower, upper\n",
    "                \n",
    "                elif pred_type == 'mean':\n",
    "                    mean = self.predict_mean()\n",
    "                    return mean, lower, upper\n",
    "                \n",
    "                elif pred_type == 'mode':\n",
    "                    mode = self.predict_mode()\n",
    "                    return mode, lower, upper\n",
    "             \n",
    "                else:\n",
    "                    median = self.predict_median(samples)\n",
    "                    mean = self.predict_mean()\n",
    "                    mode = self.predict_mode()\n",
    "                    \n",
    "                    return median, mean, mode, lower, upper\n",
    "                    \n",
    "        else:\n",
    "            raise NotImplementedError('Likelihood not implemented')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.constraints import Positive\n",
    "from kernels import Kernel\n",
    "from likelihoods import BetaLikelihood_MeanParametrization, MultitaskBetaLikelihood\n",
    "from gpytorch.means import ZeroMean\n",
    "\n",
    "\n",
    "# input for hadamard model\n",
    "dict_input = {'input' : [],\n",
    "              'output' : [],\n",
    "              'task_indices' : []}\n",
    "\n",
    "# containts the data for each task at same time intervals\n",
    "for i, (X, y) in enumerate(loader):\n",
    "    n = X.shape[0]\n",
    "    dict_input['input'].append(X)\n",
    "    dict_input['task_indices'].append(torch.ones(n, dtype=torch.long) * i)\n",
    "    dict_input['output'].append(y)\n",
    "\n",
    "task_indices = torch.cat(dict_input['task_indices'])\n",
    "x = torch.cat(dict_input['input'], dim=0)\n",
    "y = torch.stack(dict_input['output'], dim=-1)\n",
    "num_latents = y.size(-1)\n",
    "\n",
    "\n",
    "kernel = Kernel(num_latent=num_latents)\n",
    "matern_base = kernel.get_matern(lengthscale_constraint=Positive(),\n",
    "                                outputscale_constraint=Positive())\n",
    "matern_quasi = kernel.get_matern(lengthscale_constraint=Positive(),\n",
    "                                 outputscale_constraint=Positive())\n",
    "periodic1 = kernel.get_periodic(lengthscale_constraint= Positive(),\n",
    "                                outputscale_constraint=Positive())\n",
    "periodic2 = kernel.get_periodic(lengthscale_constraint= Positive(),\n",
    "                                outputscale_constraint=Positive()\n",
    "                                )\n",
    "\n",
    "covar_module = kernel.get_quasi_periodic(matern_base=matern_base, \n",
    "                                    matern_quasi=matern_quasi,\n",
    "                                    periodic1=periodic1,\n",
    "                                    periodic2=None)\n",
    "\n",
    "likelihood = MultitaskBetaLikelihood(num_tasks=num_latents)\n",
    "mean_module = ZeroMean(batch_shape=torch.Size([num_latents]))\n",
    "\n",
    "# TODO should have task indices specified in forward method\n",
    "# TODO should have index kernel for task covariance?\n",
    "# TODO one dim or multi dim likelihood?\n",
    "# TODO independent multitask variational strategy?\n",
    "# TODO make class that satisfies above requirements\n",
    "\n",
    "model = MultitaskGPModel(X=x,\n",
    "                         y=y,\n",
    "                         likelihood=likelihood,\n",
    "                         mean_module=mean_module,\n",
    "                         covar_module=covar_module,\n",
    "                         num_latents=y.size(-1),\n",
    "                         task_indices=task_indices)\n",
    "pred = model(x, task_indices=task_indices)\n",
    "# model.fit(n_iter=100, lr=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, tensor(8))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_latents, task_indices.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([640, 5]), torch.Size([80, 8]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
