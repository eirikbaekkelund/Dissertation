{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from data import PVWeatherLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parameters\n",
    "DAY_INIT = 40\n",
    "DAY_MIN = 8\n",
    "DAY_MAX = 16\n",
    "N_DAYS = 10\n",
    "MINUTE_INTERVAL = 60\n",
    "DAILY_DATA_POINTS = (DAY_MAX - DAY_MIN) * 60 / MINUTE_INTERVAL\n",
    "N_SYSTEMS = 10\n",
    "# create a tuple of 4 coordinates that form a polygon in the uk\n",
    "# and a circle with a radius of r\n",
    "CIRCLE_COORDS = (55, -1.5)\n",
    "RADIUS = 0.3\n",
    "POLY_COORDS = ((50, -6), (50.5, 1.9), (57.6, -5.5), (58, 1.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading data\n",
      "==> Loaded data in: 0 m : 1 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eirikbaekkelund/Desktop/UCL/Code/Dissertation/data/utils.py:548: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop('season', axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "loader = PVWeatherLoader(\n",
    "    # number of days to get data for\n",
    "    n_days=N_DAYS,\n",
    "    # initial day of the data for that season\n",
    "    # look at the data frame to see which day it is\n",
    "    day_init=DAY_INIT,\n",
    "    # number of systems to extract\n",
    "    n_systems=N_SYSTEMS,\n",
    "    coords=CIRCLE_COORDS,\n",
    "    radius=RADIUS,\n",
    "    # the minute interval our data is sampled at \n",
    "    # (e.g. 60 for hourly, 30 for half hourly, 15 for 15 minutes, etc.)\n",
    "    minute_interval=MINUTE_INTERVAL,\n",
    "    # the minimum and maximum hour of the day to use\n",
    "    # (e.g. 8 and 15 for 8am to 3pm)\n",
    "    day_min=DAY_MIN,\n",
    "    day_max=DAY_MAX,\n",
    "    folder_name='pv_data',\n",
    "    file_name='pv_and_weather.csv',\n",
    "    distance_method='circle',\n",
    "    # optionally use a season\n",
    "    season='winter',\n",
    "    # optionally drop series with nan values\n",
    "    drop_nan=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from torch.nn import Parameter\n",
    "from gpytorch.distributions import base_distributions\n",
    "from gpytorch.constraints import Positive, Interval\n",
    "from gpytorch.priors import Prior\n",
    "from typing import Optional\n",
    "\n",
    "class BetaLikelihood_MeanParametrization(gpytorch.likelihoods.BetaLikelihood):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 scale : Optional[torch.Tensor] = 30,\n",
    "                 correcting_scale  : Optional[float] = 1,\n",
    "                 correcting_scale_lower_bound : Optional[float] = 0.1,\n",
    "                 correcting_scale_upper_bound : Optional[float] = 0.9,\n",
    "                 *args, **kwargs):\n",
    "        \n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        assert scale > 0, 'scale must be positive'\n",
    "        assert correcting_scale > 0, 'scale must be positive'\n",
    "        assert 0 <= correcting_scale_lower_bound <= 1, 'lower bound must be in [0, 1]'\n",
    "        assert 0 <= correcting_scale_upper_bound <= 1, 'upper bound must be in [0, 1]'\n",
    "        assert correcting_scale_lower_bound < correcting_scale_upper_bound, 'lower bound must be smaller than upper bound'\n",
    "        \n",
    "        self.scale = scale        \n",
    "        self.correcting_scale = Parameter(torch.tensor(correcting_scale, dtype=torch.float64), \n",
    "                                            requires_grad=False)\n",
    "\n",
    "    \n",
    "    def forward(self, function_samples, *args, **kwargs):\n",
    "        \n",
    "        mixture = torch.distributions.Normal(0, 1).cdf(function_samples)\n",
    "        print(mixture.shape)\n",
    "        print(self.scale.shape)\n",
    "        \n",
    "        alpha = mixture * self.scale \n",
    "        beta = self.scale - alpha \n",
    "\n",
    "        print(mixture.shape)\n",
    "        print(alpha.shape)\n",
    "        print(beta.shape)\n",
    "        \n",
    "        self.alpha = torch.clamp(alpha, 1e-10, 1e10)\n",
    "        self.beta = torch.clamp(beta, 1e-10, 1e10)\n",
    "\n",
    "        return base_distributions.Beta(concentration1=self.alpha, concentration0=self.beta)\n",
    "    \n",
    "    def mode(self):\n",
    "        \"\"\" \n",
    "        Calculate the mode of a beta distribution given the alpha and beta parameters\n",
    "\n",
    "        Args:\n",
    "            alpha (torch.Tensor): alpha parameter\n",
    "            beta (torch.Tensor): beta parameter\n",
    "        \n",
    "        Returns:\n",
    "            result (torch.Tensor): modes of the beta distribution drawn from MC samples\n",
    "        \"\"\"\n",
    "        # detach alpha and beta from the graph\n",
    "        result = np.zeros_like(self.alpha)  # Initialize an array of zeros with the same shape as alpha\n",
    "\n",
    "        mask_alpha_gt_1 = self.alpha > 1\n",
    "        mask_beta_gt_1 = self.beta > 1\n",
    "        mask_alpha_eq_beta = self.alpha == self.beta\n",
    "        mask_alpha_le_1 = self.alpha <= 1\n",
    "        mask_beta_le_1 = self.beta <= 1\n",
    "\n",
    "        result[mask_alpha_gt_1 & mask_beta_gt_1] = (self.alpha[mask_alpha_gt_1 & mask_beta_gt_1] - 1) / (self.alpha[mask_alpha_gt_1 & mask_beta_gt_1] + self.beta[mask_alpha_gt_1 & mask_beta_gt_1] - 2)\n",
    "        result[mask_alpha_eq_beta] = 0.5\n",
    "        result[mask_alpha_le_1 & mask_beta_gt_1] = 0\n",
    "        result[mask_alpha_gt_1 & mask_beta_le_1] = 1\n",
    "\n",
    "        return result\n",
    "\n",
    "class MultitaskBetaLikelihood(BetaLikelihood_MeanParametrization):\n",
    "    \"\"\" \n",
    "    A multitask BetaLikelihood that supports multitask GP regression.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tasks: int,\n",
    "        scale = 15,\n",
    "        correcting_scale = 1,\n",
    "        batch_shape: torch.Size = torch.Size([]),\n",
    "        scale_prior: Optional[Prior] = None,\n",
    "        scale_constraint: Optional[Interval] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(scale=scale, correcting_scale=correcting_scale)\n",
    "\n",
    "        if scale_constraint is None:\n",
    "            scale_constraint = Positive()\n",
    "\n",
    "        self.raw_scale = torch.nn.Parameter(torch.ones(*batch_shape, 1, num_tasks))\n",
    "        if scale_prior is not None:\n",
    "            self.register_prior(\"scale_prior\", scale_prior, lambda m: m.scale, lambda m, v: m._set_scale(v))\n",
    "\n",
    "        self.register_constraint(\"raw_scale\", scale_constraint)\n",
    "\n",
    "    def expected_log_prob(self, observations, function_dist, *args, **kwargs):\n",
    "        ret = super().expected_log_prob(observations, function_dist, *args, **kwargs)\n",
    "        \n",
    "        num_event_dim = len(function_dist.event_shape)\n",
    "        \n",
    "        if num_event_dim > 1:  # Do appropriate summation for multitask likelihood\n",
    "            ret = ret.sum(list(range(-1, -num_event_dim, -1)))\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.variational import LMCVariationalStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tasks = 8\n",
    "from gpytorch.kernels import IndexKernel\n",
    "\n",
    "class IndependentMultitaskGPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, x_train, y_train, num_tasks):\n",
    "        # Let's use a different set of inducing points for each task\n",
    "        y_train = y_train\n",
    "\n",
    "        # We have to mark the CholeskyVariationalDistribution as batch\n",
    "        # so that we learn a variational distribution for each task\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            x_train.size(-2), batch_shape=torch.Size([num_tasks])\n",
    "        )\n",
    "\n",
    "        variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, x_train, variational_distribution, learn_inducing_locations=True\n",
    "            ),\n",
    "            num_tasks=num_tasks,\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        # The mean and covariance modules should be marked as batch\n",
    "        # so we learn a different set of hyperparameters\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([num_tasks]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_tasks])),\n",
    "            batch_shape=torch.Size([num_tasks])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        mean_x = self.mean_module(x)\n",
    "        \n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.constraints import Positive\n",
    "from kernels import get_mean_covar\n",
    "from gpytorch.means import ZeroMean\n",
    "\n",
    "# input for hadamard model\n",
    "dict_input = {'input' : [],\n",
    "              'output' : [],\n",
    "              'task_indices' : []}\n",
    "\n",
    "# containts the data for each task at same time intervals\n",
    "for i, (X, y) in enumerate(loader):\n",
    "    n = X.shape[0]\n",
    "    dict_input['input'].append(X)\n",
    "    task_index = torch.ones(n, dtype=torch.long) * i\n",
    "    dict_input['task_indices'].append(task_index)\n",
    "    dict_input['output'].append(y)\n",
    "\n",
    "task_indices = torch.cat(dict_input['task_indices'])\n",
    "x = torch.cat(dict_input['input'], dim=0)\n",
    "y = torch.stack(dict_input['output'], dim=-1)\n",
    "\n",
    "num_tasks = 8\n",
    "mean, covar = get_mean_covar(num_latent=num_tasks)\n",
    "likelihood = MultitaskBetaLikelihood(batch_shape=torch.Size([num_tasks]), num_tasks=y.size(-1))\n",
    "\n",
    "# TODO should have task indices specified in forward method\n",
    "# TODO should have index kernel for task covariance?\n",
    "# TODO one dim or multi dim likelihood?\n",
    "# TODO independent multitask variational strategy?\n",
    "# TODO make class that satisfies above requirements\n",
    "\n",
    "model = IndependentMultitaskGPModel(x, y, num_tasks=num_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([640, 5])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method TorchDistributionMixin.shape of MultivariateNormal(loc: torch.Size([640]))>\n",
      "torch.Size([20, 640])\n",
      "torch.Size([8, 1, 8])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (640) must match the size of tensor b (8) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m output_dist \u001b[39m=\u001b[39m model(x, task_indices\u001b[39m=\u001b[39mtask_indices)\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(output_dist\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 11\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mmll(output_dist, y)\n\u001b[1;32m     12\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mIter \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m - Loss: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m, loss\u001b[39m.\u001b[39mitem()))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gp/lib/python3.11/site-packages/gpytorch/module.py:30\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 30\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     32\u001b[0m         \u001b[39mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gp/lib/python3.11/site-packages/gpytorch/mlls/variational_elbo.py:77\u001b[0m, in \u001b[0;36mVariationalELBO.forward\u001b[0;34m(self, variational_dist_f, target, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, variational_dist_f, target, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     64\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m    Computes the Variational ELBO given :math:`q(\\mathbf f)` and :math:`\\mathbf y`.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m    Calling this function will call the likelihood's :meth:`~gpytorch.likelihoods.Likelihood.expected_log_prob`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39m    :return: Variational ELBO. Output shape corresponds to batch shape of the model/input data.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(variational_dist_f, target, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gp/lib/python3.11/site-packages/gpytorch/mlls/_approximate_mll.py:58\u001b[0m, in \u001b[0;36m_ApproximateMarginalLogLikelihood.forward\u001b[0;34m(self, approximate_dist_f, target, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39m# Get likelihood term and KL term\u001b[39;00m\n\u001b[1;32m     57\u001b[0m num_batch \u001b[39m=\u001b[39m approximate_dist_f\u001b[39m.\u001b[39mevent_shape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 58\u001b[0m log_likelihood \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_likelihood_term(approximate_dist_f, target, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mdiv(num_batch)\n\u001b[1;32m     59\u001b[0m kl_divergence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mvariational_strategy\u001b[39m.\u001b[39mkl_divergence()\u001b[39m.\u001b[39mdiv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_data \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta)\n\u001b[1;32m     61\u001b[0m \u001b[39m# Add any additional registered loss terms\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gp/lib/python3.11/site-packages/gpytorch/mlls/variational_elbo.py:61\u001b[0m, in \u001b[0;36mVariationalELBO._log_likelihood_term\u001b[0;34m(self, variational_dist_f, target, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_log_likelihood_term\u001b[39m(\u001b[39mself\u001b[39m, variational_dist_f, target, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlikelihood\u001b[39m.\u001b[39;49mexpected_log_prob(target, variational_dist_f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 102\u001b[0m, in \u001b[0;36mMultitaskBetaLikelihood.expected_log_prob\u001b[0;34m(self, observations, function_dist, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexpected_log_prob\u001b[39m(\u001b[39mself\u001b[39m, observations, function_dist, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 102\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mexpected_log_prob(observations, function_dist, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    104\u001b[0m     num_event_dim \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(function_dist\u001b[39m.\u001b[39mevent_shape)\n\u001b[1;32m    106\u001b[0m     \u001b[39mif\u001b[39;00m num_event_dim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:  \u001b[39m# Do appropriate summation for multitask likelihood\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gp/lib/python3.11/site-packages/gpytorch/likelihoods/likelihood.py:343\u001b[0m, in \u001b[0;36m_OneDimensionalLikelihood.expected_log_prob\u001b[0;34m(self, observations, function_dist, *args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexpected_log_prob\u001b[39m(\u001b[39mself\u001b[39m, observations, function_dist, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    342\u001b[0m     log_prob_lambda \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m function_samples: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(function_samples)\u001b[39m.\u001b[39mlog_prob(observations)\n\u001b[0;32m--> 343\u001b[0m     log_prob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquadrature(log_prob_lambda, function_dist)\n\u001b[1;32m    344\u001b[0m     \u001b[39mreturn\u001b[39;00m log_prob\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gp/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gp/lib/python3.11/site-packages/gpytorch/utils/quadrature.py:81\u001b[0m, in \u001b[0;36mGaussHermiteQuadrature1D.forward\u001b[0;34m(self, func, gaussian_dists)\u001b[0m\n\u001b[1;32m     78\u001b[0m locations \u001b[39m=\u001b[39m _pad_with_singletons(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocations, num_singletons_before\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, num_singletons_after\u001b[39m=\u001b[39mmeans\u001b[39m.\u001b[39mdim())\n\u001b[1;32m     80\u001b[0m shifted_locs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqrt(\u001b[39m2.0\u001b[39m \u001b[39m*\u001b[39m variances) \u001b[39m*\u001b[39m locations \u001b[39m+\u001b[39m means\n\u001b[0;32m---> 81\u001b[0m log_probs \u001b[39m=\u001b[39m func(shifted_locs)\n\u001b[1;32m     82\u001b[0m weights \u001b[39m=\u001b[39m _pad_with_singletons(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights, num_singletons_before\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, num_singletons_after\u001b[39m=\u001b[39mlog_probs\u001b[39m.\u001b[39mdim() \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     84\u001b[0m res \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(math\u001b[39m.\u001b[39mpi)) \u001b[39m*\u001b[39m (log_probs \u001b[39m*\u001b[39m weights)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gp/lib/python3.11/site-packages/gpytorch/likelihoods/likelihood.py:342\u001b[0m, in \u001b[0;36m_OneDimensionalLikelihood.expected_log_prob.<locals>.<lambda>\u001b[0;34m(function_samples)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexpected_log_prob\u001b[39m(\u001b[39mself\u001b[39m, observations, function_dist, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 342\u001b[0m     log_prob_lambda \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m function_samples: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(function_samples)\u001b[39m.\u001b[39mlog_prob(observations)\n\u001b[1;32m    343\u001b[0m     log_prob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquadrature(log_prob_lambda, function_dist)\n\u001b[1;32m    344\u001b[0m     \u001b[39mreturn\u001b[39;00m log_prob\n",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m, in \u001b[0;36mBetaLikelihood_MeanParametrization.forward\u001b[0;34m(self, function_samples, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mprint\u001b[39m(mixture\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     36\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 38\u001b[0m alpha \u001b[39m=\u001b[39m mixture \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale \n\u001b[1;32m     39\u001b[0m beta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale \u001b[39m-\u001b[39m alpha \n\u001b[1;32m     41\u001b[0m \u001b[39mprint\u001b[39m(mixture\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (640) must match the size of tensor b (8) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=y.size(0))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    output_dist = model(x, task_indices=task_indices)\n",
    "    print(output_dist.shape)\n",
    "    loss = -mll(output_dist, y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, 10, loss.item()))\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
